# 统计学习方法笔记 （李航著）
## 概率论与随机过程
### 1. 概率与分布

1.条件概率：已知 \\(A\\) 事件发生的条件下 \\(B\\) 发生的概率，记作 \\(P(B/A)\\)，它等于事件 \\(AB\\) 的概率相对于事件 \\(A\\) 的概率，即： \\(P(B/A)=\frac {P(AB)}{P(A)}\\),其中必须有  \\(P(A) \gt 0\\)

2.独立事件：如果 \\(P(AB)=P(A)P(B) \\) 则称两个事件  \\(A,B\\) 是独立的。不独立的两个事件称之为相依的。

3.定义\\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\)  的联合分布为：
\\( P(a,b)=P\\{X \le a, Y \le b\\}, - \infty \lt a,b \lt + \infty \\)  

\\(\mathrm{X}\\) 的分布可以从联合分布中得到：
 
\\( P_X(a)=P\\{X \le a\\}=P\\{X \le a, Y \le \infty\\}=P(a,\infty), - \infty \lt a \lt + \infty \\)

类似的，\\(\mathrm{Y}\\) 的分布可以从联合分布中得到：

\\( P_Y(b)=P\\{Y \le b\\}=P\\{X \le \infty, Y \le b\\}=P(\infty,b), - \infty \lt b \lt + \infty \\)

- 当 \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 都是离散随机变量时， 定义 \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 的联合概率质量函数为： \\(p(x,y)=P\\{X=x,Y=y\\}\\)，则  \\(\mathrm{X}\\) 和  \\(\mathrm{Y}\\) 的概率质量函数分布为：

$$
p_X(x)=\sum_{y \;:\;p(x,y) \gt 0}p(x,y) \\\
p_Y(y)=\sum\_{x \;:\;p(x,y) \gt 0}p(x,y)
$$ 

- 当 \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 联合地连续时，即存在函数 \\(f(x,y)\\) ，使得对于所有的实数集合 \\(A\\) 和  \\(B\\) 满足：  \\(P\\{X \in A, Y \in B\\}=\int_B \int_A f(x,y) dx dy\\)，则函数  \\(f(x,y)\\) 称为 \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 的概率密度函数。 

	联合分布为\\(P(a,b)=P\\{X \le a, Y \le b\\}=	\int_{-\infty}^{a} \int\_{-\infty}^{b} f(x,y) dx dy\\)。\\(\mathrm{X}\\) 和  \\(\mathrm{Y}\\) 的概率密度函数以及分布函数粉分别为：

$$
P_X(a)=\int_{-\infty}^{a} \int\_{-\infty}^{\infty} f(x,y) dx dy =\int\_{-\infty}^{a} f_X(x)dx\\\
P_Y(b)=\int\_{-\infty}^{\infty} \int\_{-\infty}^{b} f(x,y) dx dy=\int\_{-\infty}^{b} f_Y(y)dy\\\
f_X(x)=\int\_{-\infty}^{\infty} f(x,y) dy\\\
f_Y(y)=\int\_{-\infty}^{\infty} f(x,y) dx
$$
### 2. 期望
1.期望：（是概率分布的泛函，函数的函数）
	
- 离散型随机变量 \\(X\\) 的期望 \\(E(X)=\sum_{i=1}^{\infty}x_ip_i\\)（若级数不收敛，则期望不存在）
- 连续性随机变量 \\(X\\) 的期望 \\(E(X)=\int_{-\infty}^{\infty}xf(x)dx\\)，（若极限不收敛，则期望不存在）
> 期望描述了随机变量的平均情况，衡量了随机变量 X 的均值

2.定理：设 \\(Y=g(X)\\) 均为随机变量，g 是连续函数

- 若 \\(X\\)  为离散型随机变量，若 Y 的期望存在，则 \\(E(Y)=E[g(X)]=\sum_{i=1}^{\infty}g(x_i)p_i\\)。
- 若 \\(X\\)  为连续型随机变量，若 Y 的期望存在，则 \\(E(Y)=E[g(X)]=\int_{-\infty}^{\infty}g(x)f(x)dx\\)。
> 该定理的意义在于：当求 E(Y) 时，不必计算出 Y 的分布，只需要利用 X 的分布即可
>
> 该定理可以推广至两个或者两个以上随机变量的情况。此时用 \\(E(Z)=E[g(X,Y)]=\int\_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f(x,y)dxdy\\)。

3.期望性质：

- 常数的期望就是常数本身
- 对常数 C 有 \\(E(CX)=CE(X)\\)
- 对两个随机变量  X，Y，有： E(X+Y)=E(X)+E(Y) ；该结论可以推广到任意有限个随机变量之和的情况
- 对两个相互独立的随机变量，有 E(XY)=E(X)E(Y)；该结论可以推广到任意有限个相互独立的随机变量之积的情况

### 3. 方差
1.对随机变量 X ，若  \\(E\\{ [X-E(X)]^{2}\\}\\) 存在，则称它为 X 的方差，记作 Var(X)。 X 的标准差为方差的开平方，即：
$$
Var(X)=E\\{ [X-E(X)]^{2}\\} \\\
\sigma=\sqrt{Var(X)}
$$
> 方差度量了随机变量 X 与期望值偏离的程度，衡量了 X  取值分散程度的一个尺度。由于绝对值 |X-E(X)| 带有绝对值，不方便运算，因此采用平方来计算。又因为 \\(|X-E(X)|^2\\) 是一个随机变量，因此对它取期望，即得 X 与期望值偏离的均值

2.根据定义可知， \\(Var(X)=E\\{ [X-E(X)]^{2}\\}=E(X^{2})-[E(X)]^{2}\\)

3.对于一个期望为  \\(\mu\\)， 方差为 \\(\sigma^{2},\sigma \ne 0\\) 的随机变量 X，随机变量  \\(X^{\ast}=\frac {X-\mu}{\sigma\}\\) 的数学期望为0，方差为1.称 \\(X^{\ast}\\) 为 X 的标准化变量

4.方差的性质：

- 常数的方差恒为0
- 对常数C有 \\(Var(CX)=C^{2}Var(X)\\)
- 对两个随机变量  X，Y，有： Var(X+Y)=Var(X)+Var(Y)+2E{(X-E(X))(Y-E(Y))}。当 X 和 Y相互独立时，有 Var(X+Y)= Var(X)+Var(Y) ，可以推广至任意有限多个相互独立的随机变量之和的情况
- Var(X)=0 的充要条件是 X 以概率1取常数

### 4. 大数定律及中心极限定理
1.切比雪夫不等式：随机变量 X 具有期望 \\(E(X)=\mu\\)， 方差 \\(Var(X)=\sigma^{2}\\) , 对于任意正数 \\(\varepsilon\\) ，不等式 $$P\\{|X-\mu| \ge \varepsilon\\} \le \frac {\sigma^{2}}{\varepsilon^{2}}$$ 成立
> 其意义是：对于距离 E(X) 足够远的地方 （距离大于等于 \\(\varepsilon\\) ），事件出现的概率是小于等于 \\( \frac {\sigma^{2}}{\varepsilon^{2}}\\)；事件出现在区间 \\([\mu-\varepsilon , \mu+\varepsilon]\\) 的概率大于  \\(1- \frac {\sigma^{2}}{\varepsilon^{2}}\\)
>
>该不等式给出了随机变量 X 在分布未知的情况下， 事件  \\(\\{|X-\mu| \le \varepsilon\\}\\) 的下限估计（。如 \\(P\\{|X-\mu| \lt 3\sigma\\} \ge 0.8889\\)

证明：
$$
P\\{|X-\mu| \ge \varepsilon\\}=\int_{|x-\mu| \ge \varepsilon}f(x)dx \le \int\_{|x-\mu| \ge \varepsilon} \frac{|x-\mu|^{2}}{\varepsilon^{2}}f(x)dx \\\
\le \frac {1}{\varepsilon^{2}}\int\_{-\infty}^{\infty}(x-\mu)^{2}f(x)dx=\frac{\sigma^{2}}{\varepsilon^{2}}
$$

## 一、 统计学习方法概论

1.统计学习的对象是数据

~~~mermaid
graph LR;
A[数据]-- 提取 -->B[数据特征];
B--建立-->C[数据模型];
C--求解-->D[模型参数];
D--应用-->E[新数据预测];
~~~

2.统计学习的前提：同类数据具有一定的统计规律。
	> 同类：具有某种共同性质的数据

3.监督学习的要素：
	
- 训练数据集合：给定的、有限的数据集合。 用于学习过程，且假设这些数据独立同分布产生。
- 学习的模型：假定数域某一类函数的集合。称之为假设空间
- 策略：应用某种评价准测（比如最小化误差率，比如最大化效用等等），从假设空间中选择一个最优的模型，使得它对于已知训练数据和未知测试数据在给定评价准则下具有最优的预测。
- 算法：用于实现最优模型的选取。

任何种类的统计学习都包含模型、策略、算法这三要素。其中模型用于建模、策略用于给出目标函数、算法给出如何求解模型参数使得目标函数最优
> 监督的含义：训练数据集是人工给出的，所以称之为监督学习

4.特征向量：每个具体的输入时一个实例，由特征向量表示；  
特征空间：所有特征向量构成的空间  
输入空间：所有输入的可能取值  
输出空间：所有输出的可能取值

- 特征空间的每一个维度对应一种特征
- 可以将输入空间等同于特征空间。但是也可以不同。通过映射，从输入中提取输出

~~~mermaid
graph LR;
A[输入空间]-- 映射 -->B[特征空间];
~~~
- 模型是定义在特征空间上的5.

5.通常输入变量用 \\(\mathrm{X}\\) 表示，输出变量用  \\(\mathrm{Y}\\) 表示。具体的输入取值记作 \\(\mathbf x\\) ，具体的输出取值记作 \\(\mathbf y\\),取值既可以是标量也可以是向量。

这里所有的向量均为列向量，其中输入实例 \\(\mathbf x\\) 的特征向量记作：

$$
\mathbf{x}=
\begin{pmatrix}
x^{(1)} \\\
x^{(2)} \\\
\vdots\\\
x^{(n)} \\\
\end{pmatrix}
$$

这里  \\(x^{(i)}\\) 为  \\(\mathbf x\\)  的第 \\(i\\) 个特征的取值，这里假设特征空间为 n 维

第  \\(i\\)  个输入记作 \\( \mathbf x_i \\)，它的意义不同于  \\(x^{(i)}\\) 

6.训练数据由输入、输出对组成。通常训练集表示为：

$$
\mathbf{T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}}
$$
测试数据也由输入、输出对组成。
> 输入、输出对又称作样本/样本点

7.输入变量 \\(\mathrm{X}\\) 和输出变量  \\(\mathrm{Y}\\) 可以是连续的，也可以是离散的。

-  \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 均为连续的：这一类问题称为回归问题
-  \\(\mathrm{Y}\\) 为离散的，且是有限的：这一类问题称之为分类问题
-  \\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\) 均为变量序列：这一类问题称为标准问题

标注问题看上去好像是在序列上的分类，但是它与分类的最大区别是：标注采用的特征是有上下文分类结果的，这个结果你是不知道的。比如你要确定一个单词的分类首先必须知道上一个单词的分类，所以必须整句话所有单词一起解，没办法一个词一个词的解。  

而分类是只需要判别当前单词的类别，不需要考虑前一个单词分配的结果。

8.监督学习假设输入与输出的随机变量\\(\mathrm{X}\\) 和 \\(\mathrm{Y}\\)  遵循联合概率分布 \\( \mathrm{P(X,Y)}\\)

- 学习过程中，假定这个联合概率分布存在，但是具体定义未知
- 训练数据与测试数据视为依联合概率分布 \\( \mathrm{P(X,Y)}\\) 独立同部分产生。

> 这是监督学习关于数据的基本假设

9.监督学习的目的在于学习一个由输入到输出的映射，该映射由模型表示。

- 模型属于由输入空间到输出空间的映射的集合（该集合就是假设空间）
- 假设空间的确定意味着学习范围的确定（如，确定假设空间就是多项式，那么确定了学习范围就是学习多项式的参数）
- 监督学习的模型可以为概率模型或者非概率模型
	- 概率模型由条件概率分布 \\(P(Y/X)\\)  表示
	- 非概率模型由决策函数 \\(Y=f(X)\\) 表示
	
	预测具体输出时，记作 \\(P(\mathbf{y/x})\\) 或者记作  \\(\mathbf y=f( \mathbf x)\\)

10.监督学习的形式化描述：

- 监督学习氛围学习和预测两个过程，由学习系统和预测系统完成
- \\(\mathbf{\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}}\\) 为 N 个样本， \\(\mathbf x_i \in  \mathscr X \\) 为输入的观测值，\\(\mathbf y_i \in  \mathscr Y \\) 是输出的观测值

	假设训练数据与测试数据是依据联合概率分布 \\(P(X,Y)\\) 独立同分布的
- 学习系统利用给定的训练集，通过学习/训练得到一个模型，表示为条件概率分布 \\(\hat P(Y/X)\\) 或者决策函数 \\(Y=\hat f(X)\\)
- 预测系统对给定的测试样本集中的输入 \\(\mathbf x_{N+1}\\) ，根据模型：

$$
\mathbf y\_{N+1}=\arg\_{\mathbf y\_{N+1}} \max \hat P(\mathbf{y\_{N+1}/x\_{N+1}})\\\
\text{or :} \quad \mathbf y\_{N+1}=\hat f(\mathbf x_{N+1})
$$

   给出  \\(\mathbf y_{N+1}\\) 
> \\(\arg\max\\) 的意思是： 求得某个参数  \\(\mathbf y_{N+1}\\)  使得   \\(P(\mathbf{y\_{N+1}/x\_{N+1}})\\)  最大，返回这个 \\(\mathbf y\_{N+1}\\) 值

11.统计学习三要素：方法=模型+策略+算法

- 模型：定义了解空间。监督学习中，模型就是要学习的条件概率分布或者决策函数。模型的假设空间包含了所有可能的条件概率分布或者决策函数，因此假设空间中的模型有无穷多个。
	- 假设空间 \\( \mathscr F=\\{f/Y=f(X)\\}\\) 为决策函数的集合。 \\(X \in \mathscr X, Y \in \mathscr Y \\) 为变量，其中 \\(\mathscr X\\) 为输入空间，  \\(\mathscr Y\\) 为输出空间
		> 通常 \\( \mathscr F\\)是由一个参数向量 \\(\vec \theta=(\theta^{(1)},\cdots,\theta^{(n)})\\) 决定的函数族。 \\( \mathscr F=\\{f/Y=f_{\vec\theta}(X),\vec\theta \in \mathbb R^{n}\\}\\)， \\(f\_{\vec\theta}\\) 只与 \\(\vec\theta\\) 有关，称 \\(\vec\theta\\) 为参数空间
	- 假设空间 \\( \mathscr F=\\{P/P(Y/X)\\}\\) 为条件概率的集合。\\(X \in \mathscr X, Y \in \mathscr Y \\) 为随机变量，其中 \\(\mathscr X\\) 为输入空间，  \\(\mathscr Y\\) 为输出空间
		> 通常 \\( \mathscr F\\)是由一个参数向量 \\(\vec \theta=(\theta^{(1)},\cdots,\theta^{(n)})\\) 决定的概率分布族。 \\( \mathscr F=\\{P/P_{\vec\theta}(Y/X),\vec\theta \in \mathbb R^{n}\\}\\)， \\(P\_{\vec\theta}\\) 只与 \\(\vec\theta\\) 有关，称 \\(\vec\theta\\) 为参数空间

- 策略：定义了优化目标。策略考虑的是按照什么样的准则学习。通常有下列准则：
	- 损失函数：对于给定的输入 \\(X\\)， 由 \\(f(X) \quad or \quad  P(Y/X)\\)  预测的输出值与 \\(Y\\) 真实值可能不一致，用损失函数度量错误的程度，记作 \\(L(Y,f(X))\\) 或者 \\(L(Y,P(Y/X))\\) 。也称作代价函数。常用的损失函数有：
		- 0-1 损失函数： \\(L(Y,f(X)=1,if \quad Y \ne f(X);L(Y,f(X)=0,if \quad Y = f(X); \\)
		- 平方损失函数： \\(L(Y,f(X))=(Y-f(X))^{2}\\)
		- 绝对损失函数： \\(L(Y,f(X))=|Y-f(X)|\\)
		- 对数损失函数： \\(L(Y,P(Y/X))=- \log P(Y/X)\\)
			> 因为 Y 已经出现，因此理论上 \\(P(Y/X) \\) 为1.如果它不为1，则说明预测有误差。越远离1，说明误差越大。
	- 风险函数：（也叫做期望损失）。通常损失函数值越小，模型就越好。但是由于模型的输入输出都是随机变量，遵从联合分布 \\(P(X,Y)\\)， 因此定义风险函数为损失函数的期望：
$$
R\_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathscr{X \times Y}}L(y,f(x))P(x,y)dxdy
$$
	其中 \\(\mathscr{X , Y}\\) 分别为输入空间和输出空间。学习的目标是选择风险函数最小的模型。
	> 求\\( R_{exp}(f)\\) 的过程中要用到 \\(P(x,y)\\) ,但是  \\(P(x,y)\\) 未知。实际上如果它已知，则可以轻而易举求得条件概率分布，也就不需要学习。因此监督学习也就成为一个病态的问题。
	- 经验风险：也叫经验损失。给定训练集 \\(\mathbf{T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}}\\)，模型 \\(f(X)\\) 关于 T 的经验风险定义为：
$$
R_{emp}(f)=\frac 1{N} \sum\_{i=1}^{N}L(\mathbf y_i,f(\mathbf x_i))
$$
	经验风险是模型在 T 上的平均损失。根据大数定律，当  \\(N \rightarrow \infty \\) 时， \\(R\_{emp}(f) \rightarrow R\_{exp}(f) \\)。但是由于现实中训练集中样本数量有限，甚至很小，所以需要对经验风险进行矫正